

Binder 内存映射和接收缓存区管理：

	驱动的缓冲区即共享内存 介绍：	

		1、通过mmap实现Binder 内存映射 。
		2、通过系统调用copy_from_user() 进行接收缓存区管理。
		两者结合起来就是管理了一个共享内存。

		Binder通信实际上是位于不同进程 的线程之间的通信。由于Linux上传统的IPC通信需要两次拷贝数据到缓冲区，浪费空间，
		所以Binder采用了只拷贝一次的方式，即将位于内核的数据缓冲区和目标进程的缓冲区 合并为一个缓冲区，就是驱动中的缓冲区。

		Binder驱动实现了mmap()系统调用，mmap()的返回值是内存映射在用户空间的地址，这段空间是由驱动管理，用户不必也不能直接访问。
		接收缓存区映射好后就可以做为缓存池接收和存放数据了。

	如何理解由【内核态的驱动】通过mmap在【用户空间】映射的缓冲区？ 这可以理解为共享内存吗？
		暂时这么理解

	
	共享内存分配：
		数据包传输的结构为：binder_transaction_data ,其结构体成员中的 data.buffer所指向的内存，不需要接收方( Server )进程提供，即开发者不用提供，
		恰恰是来自mmap()映射的这片缓冲区(内存区域)。数据从A进程发送到B进程时，首先发送到驱动，驱动会根据 数据包的大小，
		使用最佳匹配算法从缓存区中分配一块 合适的空间，将数据从A进程的发送缓存区复制过来，即复制到了共享内存。

	共享内存释放：
		有分配必然有释放。接收方在处理完数据包后，就要通知驱动释放data.buffer所指向的内存区，由Binder协议的BC_FREE_BUFFER命令完成。


	
	驱动做了什么？

		驱动为接收方分担了最为繁琐的任务：分配/释放大小不等，难以预测的用来存放数据的 缓冲区 .
		接收方只要提供一个用来【存放消息头的】 【空间大小固定的】 缓冲区 即可。

	效率：
		由于mmap()分配的内存是映射在接收方用户空间里的，所以相当于对数据做了一次【从发送方用户空间】到【接收方用户空间】的【直接数据拷贝】。
		省去了内核中暂存这个步骤，相比传统IPC，提升了1倍性能。


	驱动对从A进程到B进程的数据进行【直接拷贝】是如何实现的？

		Linux中没有直接拷贝的函数，只有：
			copy_from_user()：从用户空间拷贝到内核
			copy_to_user() ：从内核拷贝到用户空间

		为了实现用户空间到用户空间的拷贝，mmap()分配的内存除了映射进了 接收方进程里，还映射进了内核空间。
		
		即直接拷贝。就是内核和用户进程 有了一个共享内存，那么调用copy_from_user()将数据拷贝进内核空间也相当于拷贝进了接收方的用户空间。


		

	
数据包接收：

	Binder通信实际上是位于不同进程中的线程之间的通信。当Client进程中的T1线程 向Server进程 发送 数据包时，Sever进程为了处理这个需要启动线程S1,
	对于Server进程S，可能有许多Client同时发起请求，为了提高效率往往开辟线程池并发处理收到的请求。

	
	Client端的发送线程只要不接收返回数据包（无返回值），则应该在全局等待队列中等待新任务，
	否则（有返回值）应该在其私有等待队列中等待Server的返回数据。

	Client端：等待返回包的线程必须是发送请求的线程，而不能由一个线程发送请求包，另一个线程等待接收包，否则将收不到返回包；
	Server端：发送对应返回数据包的线程必须是收到请求数据包的线程，否则返回的数据包将无法送交发送请求的线程。



Binder驱动是如何递交同步交互和异步交互的？

	同步交互和异步交互的区别：

		同步交互：有返回值，要等待应答端（Server）的返回数据包。
		异步交互：无返回值，发出请求数据包后交互即结束


		同步交互有限，异步限流。。。

